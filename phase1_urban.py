import argparse
import json
import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score,
    mean_absolute_error,
    confusion_matrix,
    f1_score
)
import matplotlib.pyplot as plt
import seaborn as sns
import random


# ============================================================
# SET SEED
# ============================================================

def set_seed(seed=42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    random.seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# ============================================================
# DATASET
# ============================================================

class SingleOutputDataset(Dataset):
    def __init__(self, X, y_single):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y_single, dtype=torch.long)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# ============================================================
# URBAN STRONG MODELS (Outputs 2, 4, 5)
# ============================================================

class Model2Strong(nn.Module):
    """Strong model for Output 2."""
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256), nn.ReLU(),
            nn.Linear(256, 128), nn.ReLU(),
            nn.Linear(128, 64), nn.ReLU(),
            nn.Linear(64, 4)
        )
    def forward(self, x): return self.net(x)


class UrbanStrongModel4(nn.Module):
    """Upgraded deep residual MLP for Output 4."""
    def __init__(self, input_dim):
        super().__init__()
        hidden = 256

        self.block1 = nn.Sequential(
            nn.Linear(input_dim, hidden),
            nn.LayerNorm(hidden),
            nn.GELU(),
            nn.Dropout(0.15),
        )
        self.block2 = nn.Sequential(
            nn.Linear(hidden, hidden),
            nn.LayerNorm(hidden),
            nn.GELU(),
            nn.Dropout(0.15),
        )
        self.block3 = nn.Sequential(
            nn.Linear(hidden, hidden),
            nn.LayerNorm(hidden),
            nn.GELU(),
            nn.Dropout(0.10),
        )
        self.out = nn.Linear(hidden, 4)

    def forward(self, x):
        x = self.block1(x)
        x = x + self.block2(x)
        x = x + self.block3(x)
        return self.out(x)


class UrbanStrongModel5(nn.Module):
    """Even deeper strong model for Output 5."""
    def __init__(self, input_dim):
        super().__init__()
        hidden = 320

        self.block1 = nn.Sequential(
            nn.Linear(input_dim, hidden),
            nn.LayerNorm(hidden),
            nn.GELU(),
            nn.Dropout(0.15),
        )
        self.block2 = nn.Sequential(
            nn.Linear(hidden, hidden),
            nn.LayerNorm(hidden),
            nn.GELU(),
            nn.Dropout(0.15),
        )
        self.block3 = nn.Sequential(
            nn.Linear(hidden, hidden),
            nn.LayerNorm(hidden),
            nn.GELU(),
            nn.Dropout(0.15),
        )
        self.block4 = nn.Sequential(
            nn.Linear(hidden, hidden // 2),
            nn.LayerNorm(hidden // 2),
            nn.GELU(),
            nn.Dropout(0.10),
        )
        self.out = nn.Linear(hidden // 2, 4)
        self.block5 = nn.Sequential(
            nn.Linear(hidden, hidden),
            nn.LayerNorm(hidden),
            nn.GELU(),
            nn.Dropout(0.15),
        )


    def forward(self, x):
        # Initial projection
        x = self.block1(x)

        # Residual block 1
        res = x
        x = self.block2(x)
        x = x + res

        # Residual block 2
        res = x
        x = self.block3(x)
        x = x + res

        # Residual block 3 (your block5)
        res = x
        x = self.block5(x)
        x = x + res

        # Dimensionality reduction block (no residual)
        x = self.block4(x)

        # Output layer
        return self.out(x)


# ============================================================
# BASELINE MODELS (Outputs 1 & 3)
# ============================================================

class Model1(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64), nn.ReLU(),
            nn.Linear(64, 32), nn.ReLU(),
            nn.Linear(32, 4)
        )
    def forward(self, x): return self.net(x)


class Model4(nn.Module):  # baseline for Output 3
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 80), nn.ReLU(),
            nn.Linear(80, 40), nn.ReLU(),
            nn.Linear(40, 4)
        )
    def forward(self, x): return self.net(x)


# ============================================================
# MODEL FACTORY
# ============================================================

MODEL_FACTORY = {
    1: Model1,
    3: Model4,
}

STRONG_MODELS = {
    2: Model2Strong,
    4: UrbanStrongModel4,
    5: UrbanStrongModel5,
}

EPOCHS_MAP = {
    5: 50,   # Output 5 benefits from more epochs
}


# ============================================================
# TRAINING
# ============================================================

def train_model(model, X_train, y_train, epochs=40, lr=0.001, batch=32):
    ds = SingleOutputDataset(X_train, y_train)
    loader = DataLoader(ds, batch_size=batch, shuffle=True)

    criterion = nn.CrossEntropyLoss()
    opt = torch.optim.Adam(model.parameters(), lr=lr)

    for ep in range(epochs):
        for xb, yb in loader:
            opt.zero_grad()
            pred = model(xb)
            loss = criterion(pred, yb)
            loss.backward()
            opt.step()

    return model


# ============================================================
# METRICS
# ============================================================

def evaluate(preds, y_true):
    acc = accuracy_score(y_true, preds)
    mae = mean_absolute_error(y_true, preds)
    mse = np.mean((y_true - preds)**2)
    rmse = np.sqrt(mse)
    f1 = f1_score(y_true, preds, average="weighted")
    return acc, mae, mse, rmse, f1


# ============================================================
# MAIN PIPELINE
# ============================================================

def run(args):

    set_seed(42)

    # Load
    X_df = pd.read_csv(args.xfile)
    Y_df = pd.read_csv(args.yfile)

    df = X_df.merge(Y_df, on="hh_id")

    feature_cols = [c for c in X_df.columns if c != "hh_id"]
    target_cols = [c for c in Y_df.columns if c != "hh_id"]

    X = df[feature_cols].values.astype(np.float32)
    Y = df[target_cols].values.astype(np.int64)

    indices = np.arange(len(df))

    # Train/test only
    X_train, X_test, Y_train, Y_test, idx_train, idx_test = train_test_split(
        X, Y, indices, test_size=0.20, random_state=42
    )

    hh_test = df.loc[idx_test, "hh_id"].values

    # Normalize
    mean = X_train.mean(axis=0)
    std = X_train.std(axis=0)
    std[std == 0] = 1

    X_train = (X_train - mean) / std
    X_test = (X_test - mean) / std

    # Outputs
    if args.output_id == "all":
        output_ids = [1,2,3,4,5]
    else:
        output_ids = [int(x) for x in args.output_id.split(",")]

    outdir = args.outdir
    os.makedirs(outdir, exist_ok=True)

    print("\n TRAINING MODELS \n")

    all_metrics = {}
    combined_preds = {}
    all_true = []
    all_pred = []
    saved_models = {}

    # Train each output head
    for out_id in output_ids:

        input_dim = X_train.shape[1]

        # strong model override
        if out_id in STRONG_MODELS:
            model = STRONG_MODELS[out_id](input_dim)
        else:
            model = MODEL_FACTORY[out_id](input_dim)

        epochs = EPOCHS_MAP.get(out_id, args.epochs)

        model = train_model(
            model,
            X_train,
            Y_train[:, out_id - 1],
            epochs=epochs,
            lr=args.lr,
            batch=args.batch
        )

        saved_models[out_id] = model.state_dict()
        torch.save(model.state_dict(), f"{outdir}/model_{out_id}.pt")

        # Inference
        with torch.no_grad():
            logits = model(torch.tensor(X_test, dtype=torch.float32))
            preds = logits.argmax(dim=1).numpy()

        true_col = Y_test[:, out_id - 1]

        pd.DataFrame({
            "pred": preds,
            "true": true_col
        }).to_csv(f"{outdir}/preds_model_{out_id}.csv", index=False)

        combined_preds[f"pred_{out_id}"] = preds
        combined_preds[f"true_{out_id}"] = true_col

        acc, mae, mse, rmse, f1 = evaluate(preds, true_col)

        all_metrics[out_id] = {
            "accuracy": float(acc),
            "mae": float(mae),
            "mse": float(mse),
            "rmse": float(rmse),
            "f1": float(f1)
        }

        all_pred.append(preds)
        all_true.append(true_col)

    # Save combined preds
    pd.DataFrame(combined_preds).to_csv(f"{outdir}/y_predicted.csv", index=False)

    # Save urban predictions vs ground truth
    urban_df = pd.DataFrame({"hh_id": hh_test})
    for out_id in output_ids:
        urban_df[f"true_{out_id}"] = combined_preds[f"true_{out_id}"]
        urban_df[f"pred_{out_id}"] = combined_preds[f"pred_{out_id}"]
    urban_df.to_csv(f"{outdir}/urban_predictions_vs_ground_truth.csv", index=False)

    torch.save(saved_models, f"{outdir}/all_models.pt")

    # Combined confusion matrix
    all_true = np.concatenate(all_true)
    all_pred = np.concatenate(all_pred)

    cm = confusion_matrix(all_true, all_pred, labels=[0,1,2,3])
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.tight_layout()
    plt.savefig(f"{outdir}/confusion_combined.png")
    plt.close()

    # Save JSON metrics
    with open(f"{outdir}/metrics_summary.json", "w") as f:
        json.dump(all_metrics, f, indent=4)

    # Terminal summary
    print("\n FINAL TEST METRICS SUMMARY ")
    from tabulate import tabulate

    table = []
    for out_id, m in all_metrics.items():
        table.append([
            out_id,
            f"{m['accuracy']:.4f}",
            f"{m['mae']:.4f}",
            f"{m['mse']:.4f}",
            f"{m['rmse']:.4f}",
            f"{m['f1']:.4f}"
        ])

    print(tabulate(
        table,
        headers=["Output", "Accuracy", "MAE", "MSE", "RMSE", "F1"],
        tablefmt="github"
    ))

    print("\n DONE \n")


# ============================================================
# CLI
# ============================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument("--xfile", type=str, required=True)
    parser.add_argument("--yfile", type=str, required=True)

    parser.add_argument("--output_id", type=str, default="all")
    parser.add_argument("--epochs", type=int, default=60)
    parser.add_argument("--lr", type=float, default=0.001)
    parser.add_argument("--batch", type=int, default=32)
    parser.add_argument("--outdir", type=str, default="results_urban")

    args = parser.parse_args()
    run(args)

